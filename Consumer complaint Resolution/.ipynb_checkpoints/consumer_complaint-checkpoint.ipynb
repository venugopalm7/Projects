{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Test and Train Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(478421, 18)\n",
      "(119606, 17)\n"
     ]
    }
   ],
   "source": [
    "train_file =r'Data\\Consumer_Complaints_train.csv'\n",
    "test_file = r'Data\\Consumer_Complaints_test_share.csv'\n",
    "\n",
    "train = pd.read_csv(train_file)\n",
    "test = pd.read_csv(test_file)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date received                        0\n",
       "Product                              0\n",
       "Sub-product                     138473\n",
       "Issue                                0\n",
       "Sub-issue                       292625\n",
       "Consumer complaint narrative    403327\n",
       "Company public response         388029\n",
       "Company                              0\n",
       "State                             3839\n",
       "ZIP code                          3848\n",
       "Tags                            411215\n",
       "Consumer consent provided?      342934\n",
       "Submitted via                        0\n",
       "Date sent to company                 0\n",
       "Company response to consumer         0\n",
       "Timely response?                     0\n",
       "Consumer disputed?                   0\n",
       "Complaint ID                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date received</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sub-product</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Sub-issue</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>Company public response</th>\n",
       "      <th>Company</th>\n",
       "      <th>State</th>\n",
       "      <th>ZIP code</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Consumer consent provided?</th>\n",
       "      <th>Submitted via</th>\n",
       "      <th>Date sent to company</th>\n",
       "      <th>Company response to consumer</th>\n",
       "      <th>Timely response?</th>\n",
       "      <th>Consumer disputed?</th>\n",
       "      <th>Complaint ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-05-15</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Billing statement</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wells Fargo &amp; Company</td>\n",
       "      <td>MI</td>\n",
       "      <td>48342</td>\n",
       "      <td>Older American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Web</td>\n",
       "      <td>2014-05-16</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>856103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-09-18</td>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>(CD) Certificate of deposit</td>\n",
       "      <td>Making/receiving payments, sending money</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Santander Bank US</td>\n",
       "      <td>PA</td>\n",
       "      <td>18042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Referral</td>\n",
       "      <td>2014-09-24</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1034666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-03-13</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Incorrect information on credit report</td>\n",
       "      <td>Account status</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Equifax</td>\n",
       "      <td>CA</td>\n",
       "      <td>92427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Referral</td>\n",
       "      <td>2014-04-03</td>\n",
       "      <td>Closed with non-monetary relief</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>756363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-07-17</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Billing statement</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My credit card statement from US Bank, XXXX. X...</td>\n",
       "      <td>Company chooses not to provide a public response</td>\n",
       "      <td>U.S. Bancorp</td>\n",
       "      <td>GA</td>\n",
       "      <td>305XX</td>\n",
       "      <td>Older American</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>2015-07-17</td>\n",
       "      <td>Closed with monetary relief</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1474177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-11-20</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Transaction issue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bank of America</td>\n",
       "      <td>MA</td>\n",
       "      <td>02127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Web</td>\n",
       "      <td>2014-11-28</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1132572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Date received                  Product                  Sub-product  \\\n",
       "0    2014-05-15              Credit card                          NaN   \n",
       "1    2014-09-18  Bank account or service  (CD) Certificate of deposit   \n",
       "2    2014-03-13         Credit reporting                          NaN   \n",
       "3    2015-07-17              Credit card                          NaN   \n",
       "4    2014-11-20              Credit card                          NaN   \n",
       "\n",
       "                                      Issue       Sub-issue  \\\n",
       "0                         Billing statement             NaN   \n",
       "1  Making/receiving payments, sending money             NaN   \n",
       "2    Incorrect information on credit report  Account status   \n",
       "3                         Billing statement             NaN   \n",
       "4                         Transaction issue             NaN   \n",
       "\n",
       "                        Consumer complaint narrative  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3  My credit card statement from US Bank, XXXX. X...   \n",
       "4                                                NaN   \n",
       "\n",
       "                            Company public response                Company  \\\n",
       "0                                               NaN  Wells Fargo & Company   \n",
       "1                                               NaN      Santander Bank US   \n",
       "2                                               NaN                Equifax   \n",
       "3  Company chooses not to provide a public response           U.S. Bancorp   \n",
       "4                                               NaN        Bank of America   \n",
       "\n",
       "  State ZIP code            Tags Consumer consent provided? Submitted via  \\\n",
       "0    MI    48342  Older American                        NaN           Web   \n",
       "1    PA    18042             NaN                        NaN      Referral   \n",
       "2    CA    92427             NaN                        NaN      Referral   \n",
       "3    GA    305XX  Older American           Consent provided           Web   \n",
       "4    MA    02127             NaN                        NaN           Web   \n",
       "\n",
       "  Date sent to company     Company response to consumer Timely response?  \\\n",
       "0           2014-05-16          Closed with explanation              Yes   \n",
       "1           2014-09-24                           Closed              Yes   \n",
       "2           2014-04-03  Closed with non-monetary relief              Yes   \n",
       "3           2015-07-17      Closed with monetary relief              Yes   \n",
       "4           2014-11-28          Closed with explanation              Yes   \n",
       "\n",
       "  Consumer disputed?  Complaint ID  \n",
       "0                 No        856103  \n",
       "1                 No       1034666  \n",
       "2                 No        756363  \n",
       "3                 No       1474177  \n",
       "4                 No       1132572  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'Consent provided', 'Other', 'Consent not provided',\n",
       "       'Consent withdrawn'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Consumer consent provided?'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping column consumer consent provided\n",
    "train.drop(['Consumer consent provided?'],inplace=True,axis=1)\n",
    "test.drop(['Consumer consent provided?'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping column Tags\n",
    "train.drop(['Tags'],inplace=True,axis=1)\n",
    "test.drop(['Tags'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date received                        0\n",
       "Product                              0\n",
       "Sub-product                     138473\n",
       "Issue                                0\n",
       "Sub-issue                       292625\n",
       "Consumer complaint narrative    403327\n",
       "Company public response         388029\n",
       "Company                              0\n",
       "State                             3839\n",
       "ZIP code                          3848\n",
       "Submitted via                        0\n",
       "Date sent to company                 0\n",
       "Company response to consumer         0\n",
       "Timely response?                     0\n",
       "Consumer disputed?                   0\n",
       "Complaint ID                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### imputing sub-product column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit card\n",
      "(57358, 1)\n",
      "(57358, 16)\n",
      "-------------------------------\n",
      "Bank account or service\n",
      "(0, 1)\n",
      "(54403, 16)\n",
      "-------------------------------\n",
      "Credit reporting\n",
      "(81115, 1)\n",
      "(81115, 16)\n",
      "-------------------------------\n",
      "Mortgage\n",
      "(0, 1)\n",
      "(156175, 16)\n",
      "-------------------------------\n",
      "Debt collection\n",
      "(0, 1)\n",
      "(86544, 16)\n",
      "-------------------------------\n",
      "Student loan\n",
      "(0, 1)\n",
      "(14918, 16)\n",
      "-------------------------------\n",
      "Consumer Loan\n",
      "(0, 1)\n",
      "(18599, 16)\n",
      "-------------------------------\n",
      "Money transfers\n",
      "(0, 1)\n",
      "(3349, 16)\n",
      "-------------------------------\n",
      "Prepaid card\n",
      "(0, 1)\n",
      "(2226, 16)\n",
      "-------------------------------\n",
      "Payday loan\n",
      "(0, 1)\n",
      "(3219, 16)\n",
      "-------------------------------\n",
      "Other financial service\n",
      "(0, 1)\n",
      "(507, 16)\n",
      "-------------------------------\n",
      "Virtual currency\n",
      "(0, 1)\n",
      "(8, 16)\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "unique_product = train['Product'].unique()\n",
    "for i in unique_product:\n",
    "    print(i)\n",
    "    condition = (train['Product']==i) & (train['Sub-product'].isnull())\n",
    "    cols = ['Sub-product']\n",
    "    print(train.loc[condition,cols].shape)\n",
    "    print(train[train['Product']==i].shape)\n",
    "    print('-------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only credit card and credit reporting has no sub product, so imputing sub-product of credit card as credit card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_sub_prd_cols = ['Credit reporting','Credit card']\n",
    "for i in missing_sub_prd_cols:\n",
    "    condition = (train['Product']==i) & (train['Sub-product'].isnull())\n",
    "    cols = ['Sub-product'] \n",
    "    train.loc[condition,cols] = i\n",
    "for i in missing_sub_prd_cols:\n",
    "    condition = (test['Product']==i) & (test['Sub-product'].isnull())\n",
    "    cols = ['Sub-product'] \n",
    "    test.loc[condition,cols] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date received                        0\n",
       "Product                              0\n",
       "Sub-product                          0\n",
       "Issue                                0\n",
       "Sub-issue                       292625\n",
       "Consumer complaint narrative    403327\n",
       "Company public response         388029\n",
       "Company                              0\n",
       "State                             3839\n",
       "ZIP code                          3848\n",
       "Submitted via                        0\n",
       "Date sent to company                 0\n",
       "Company response to consumer         0\n",
       "Timely response?                     0\n",
       "Consumer disputed?                   0\n",
       "Complaint ID                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date received                        0\n",
       "Product                              0\n",
       "Sub-product                          0\n",
       "Issue                                0\n",
       "Sub-issue                        73060\n",
       "Consumer complaint narrative    101049\n",
       "Company public response          96830\n",
       "Company                              0\n",
       "State                              925\n",
       "ZIP code                           926\n",
       "Submitted via                        1\n",
       "Date sent to company                 0\n",
       "Company response to consumer         0\n",
       "Timely response?                     0\n",
       "Complaint ID                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Issue column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Billing statement', 'Making/receiving payments, sending money',\n",
       "       'Incorrect information on credit report', 'Transaction issue',\n",
       "       'Loan modification,collection,foreclosure',\n",
       "       'Loan servicing, payments, escrow account',\n",
       "       'Credit card protection / Debt protection',\n",
       "       'Deposits and withdrawals',\n",
       "       \"Cont'd attempts collect debt not owed\", 'Getting a loan', 'Other',\n",
       "       'Account opening, closing, or management',\n",
       "       \"Credit reporting company's investigation\",\n",
       "       'Improper use of my credit report', 'Managing the loan or lease',\n",
       "       'Communication tactics', 'False statements or representation',\n",
       "       'Dealing with my lender or servicer',\n",
       "       'Problems caused by my funds being low', 'Delinquent account',\n",
       "       'Identity theft / Fraud / Embezzlement',\n",
       "       'Other transaction issues', 'Disclosure verification of debt',\n",
       "       'Balance transfer fee', 'Using a debit or ATM card',\n",
       "       'Improper contact or sharing of info',\n",
       "       'Application, originator, mortgage broker', 'Other fee',\n",
       "       'Advertising and marketing', 'Settlement process and costs',\n",
       "       'Taking/threatening an illegal action', 'Late fee',\n",
       "       'APR or interest rate', 'Fraud or scam', 'Billing disputes',\n",
       "       'Problems when you are unable to pay',\n",
       "       'Credit monitoring or identity protection',\n",
       "       'Taking out the loan or lease', 'Application processing delay',\n",
       "       \"Charged fees or interest I didn't expect\", 'Repaying your loan',\n",
       "       'Customer service / Customer relations',\n",
       "       'Managing, opening, or closing account', 'Privacy',\n",
       "       'Shopping for a loan or lease', 'Closing/Cancelling account',\n",
       "       'Credit reporting', \"Can't repay my loan\", 'Rewards',\n",
       "       'Payoff process', 'Credit line increase/decrease',\n",
       "       \"Can't contact lender\", 'Credit decision / Underwriting',\n",
       "       'Unable to get credit report/credit score',\n",
       "       'Collection debt dispute', 'Credit determination', 'Bankruptcy',\n",
       "       'Account terms and changes', 'Shopping for a line of credit',\n",
       "       'Wrong amount charged or received', 'Forbearance / Workout plans',\n",
       "       'Collection practices', 'Unauthorized transactions/trans. issues',\n",
       "       'Adding money', 'Unsolicited issuance of credit card',\n",
       "       'Money was not available when promised',\n",
       "       \"Received a loan I didn't apply for\", 'Excessive fees',\n",
       "       'Applied for loan/did not receive money', 'Arbitration', 'Fees',\n",
       "       'Incorrect/missing disclosures or info',\n",
       "       'Customer service/Customer relations',\n",
       "       'Managing the line of credit', 'Overlimit fee', 'Balance transfer',\n",
       "       \"Can't stop charges to bank account\", 'Unexpected/Other fees',\n",
       "       'Other service issues', 'Cash advance',\n",
       "       'Charged bank acct wrong day or amt',\n",
       "       'Payment to acct not credited', 'Sale of account',\n",
       "       'Cash advance fee', 'Convenience checks', 'Disclosures',\n",
       "       'Advertising, marketing or disclosures',\n",
       "       'Lender damaged or destroyed vehicle',\n",
       "       'Overdraft, savings or rewards features', 'Lost or stolen check',\n",
       "       'Incorrect exchange rate', 'Lost or stolen money order',\n",
       "       'Lender repossessed or sold the vehicle',\n",
       "       'Lender sold the property', 'Lender damaged or destroyed property'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Issue'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = (train['Issue'].isnull())\n",
    "cols = ['Issue']\n",
    "train.loc[condition,cols] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemma = WordNetLemmatizer()\n",
    "my_stop = set(stopwords.words('english')+list(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_lemmas(message):\n",
    "    message = message.lower()\n",
    "    words = word_tokenize(message)\n",
    "    words_sans_stop = []\n",
    "    for word in words:\n",
    "        if word in my_stop:\n",
    "            continue\n",
    "        words_sans_stop.append(word)\n",
    "    return[lemma.lemmatize(word) for word in words_sans_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer = split_into_lemmas,\n",
    "                        min_df=0.1,\n",
    "                        max_df=0.8,\n",
    "                        stop_words=my_stop)\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_issue = tfidf.fit(train['Issue'])\n",
    "train_tfidf = x_issue.transform(train['Issue'])\n",
    "test_tfidf = x_issue.transform(test['Issue'])\n",
    "\n",
    "train_features1 = pd.DataFrame(train_tfidf.toarray())\n",
    "test_features1 = pd.DataFrame(test_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(478421, 13)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TfidfVectorizer in module sklearn.feature_extraction.text:\n",
      "\n",
      "class TfidfVectorizer(CountVectorizer)\n",
      " |  TfidfVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer='word', stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.float64'>, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
      " |  \n",
      " |  Convert a collection of raw documents to a matrix of TF-IDF features.\n",
      " |  \n",
      " |  Equivalent to :class:`CountVectorizer` followed by\n",
      " |  :class:`TfidfTransformer`.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  input : string {'filename', 'file', 'content'}\n",
      " |      If 'filename', the sequence passed as an argument to fit is\n",
      " |      expected to be a list of filenames that need reading to fetch\n",
      " |      the raw content to analyze.\n",
      " |  \n",
      " |      If 'file', the sequence items must have a 'read' method (file-like\n",
      " |      object) that is called to fetch the bytes in memory.\n",
      " |  \n",
      " |      Otherwise the input is expected to be the sequence strings or\n",
      " |      bytes items are expected to be analyzed directly.\n",
      " |  \n",
      " |  encoding : string, 'utf-8' by default.\n",
      " |      If bytes or files are given to analyze, this encoding is used to\n",
      " |      decode.\n",
      " |  \n",
      " |  decode_error : {'strict', 'ignore', 'replace'} (default='strict')\n",
      " |      Instruction on what to do if a byte sequence is given to analyze that\n",
      " |      contains characters not of the given `encoding`. By default, it is\n",
      " |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      " |      values are 'ignore' and 'replace'.\n",
      " |  \n",
      " |  strip_accents : {'ascii', 'unicode', None} (default=None)\n",
      " |      Remove accents and perform other character normalization\n",
      " |      during the preprocessing step.\n",
      " |      'ascii' is a fast method that only works on characters that have\n",
      " |      an direct ASCII mapping.\n",
      " |      'unicode' is a slightly slower method that works on any characters.\n",
      " |      None (default) does nothing.\n",
      " |  \n",
      " |      Both 'ascii' and 'unicode' use NFKD normalization from\n",
      " |      :func:`unicodedata.normalize`.\n",
      " |  \n",
      " |  lowercase : boolean (default=True)\n",
      " |      Convert all characters to lowercase before tokenizing.\n",
      " |  \n",
      " |  preprocessor : callable or None (default=None)\n",
      " |      Override the preprocessing (string transformation) stage while\n",
      " |      preserving the tokenizing and n-grams generation steps.\n",
      " |  \n",
      " |  tokenizer : callable or None (default=None)\n",
      " |      Override the string tokenization step while preserving the\n",
      " |      preprocessing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |  analyzer : string, {'word', 'char', 'char_wb'} or callable\n",
      " |      Whether the feature should be made of word or character n-grams.\n",
      " |      Option 'char_wb' creates character n-grams only from text inside\n",
      " |      word boundaries; n-grams at the edges of words are padded with space.\n",
      " |  \n",
      " |      If a callable is passed it is used to extract the sequence of features\n",
      " |      out of the raw, unprocessed input.\n",
      " |  \n",
      " |  stop_words : string {'english'}, list, or None (default=None)\n",
      " |      If a string, it is passed to _check_stop_list and the appropriate stop\n",
      " |      list is returned. 'english' is currently the only supported string\n",
      " |      value.\n",
      " |      There are several known issues with 'english' and you should\n",
      " |      consider an alternative (see :ref:`stop_words`).\n",
      " |  \n",
      " |      If a list, that list is assumed to contain stop words, all of which\n",
      " |      will be removed from the resulting tokens.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |      If None, no stop words will be used. max_df can be set to a value\n",
      " |      in the range [0.7, 1.0) to automatically detect and filter stop\n",
      " |      words based on intra corpus document frequency of terms.\n",
      " |  \n",
      " |  token_pattern : string\n",
      " |      Regular expression denoting what constitutes a \"token\", only used\n",
      " |      if ``analyzer == 'word'``. The default regexp selects tokens of 2\n",
      " |      or more alphanumeric characters (punctuation is completely ignored\n",
      " |      and always treated as a token separator).\n",
      " |  \n",
      " |  ngram_range : tuple (min_n, max_n) (default=(1, 1))\n",
      " |      The lower and upper boundary of the range of n-values for different\n",
      " |      n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
      " |      will be used.\n",
      " |  \n",
      " |  max_df : float in range [0.0, 1.0] or int (default=1.0)\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly higher than the given threshold (corpus-specific\n",
      " |      stop words).\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  min_df : float in range [0.0, 1.0] or int (default=1)\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly lower than the given threshold. This value is also\n",
      " |      called cut-off in the literature.\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  max_features : int or None (default=None)\n",
      " |      If not None, build a vocabulary that only consider the top\n",
      " |      max_features ordered by term frequency across the corpus.\n",
      " |  \n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  vocabulary : Mapping or iterable, optional (default=None)\n",
      " |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      " |      indices in the feature matrix, or an iterable over terms. If not\n",
      " |      given, a vocabulary is determined from the input documents.\n",
      " |  \n",
      " |  binary : boolean (default=False)\n",
      " |      If True, all non-zero term counts are set to 1. This does not mean\n",
      " |      outputs will have only 0/1 values, only that the tf term in tf-idf\n",
      " |      is binary. (Set idf and normalization to False to get 0/1 outputs.)\n",
      " |  \n",
      " |  dtype : type, optional (default=float64)\n",
      " |      Type of the matrix returned by fit_transform() or transform().\n",
      " |  \n",
      " |  norm : 'l1', 'l2' or None, optional (default='l2')\n",
      " |      Each output row will have unit norm, either:\n",
      " |      * 'l2': Sum of squares of vector elements is 1. The cosine\n",
      " |      similarity between two vectors is their dot product when l2 norm has\n",
      " |      been applied.\n",
      " |      * 'l1': Sum of absolute values of vector elements is 1.\n",
      " |      See :func:`preprocessing.normalize`\n",
      " |  \n",
      " |  use_idf : boolean (default=True)\n",
      " |      Enable inverse-document-frequency reweighting.\n",
      " |  \n",
      " |  smooth_idf : boolean (default=True)\n",
      " |      Smooth idf weights by adding one to document frequencies, as if an\n",
      " |      extra document was seen containing every term in the collection\n",
      " |      exactly once. Prevents zero divisions.\n",
      " |  \n",
      " |  sublinear_tf : boolean (default=False)\n",
      " |      Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  vocabulary_ : dict\n",
      " |      A mapping of terms to feature indices.\n",
      " |  \n",
      " |  idf_ : array, shape (n_features)\n",
      " |      The inverse document frequency (IDF) vector; only defined\n",
      " |      if  ``use_idf`` is True.\n",
      " |  \n",
      " |  stop_words_ : set\n",
      " |      Terms that were ignored because they either:\n",
      " |  \n",
      " |        - occurred in too many documents (`max_df`)\n",
      " |        - occurred in too few documents (`min_df`)\n",
      " |        - were cut off by feature selection (`max_features`).\n",
      " |  \n",
      " |      This is only available if no vocabulary was given.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      " |  >>> corpus = [\n",
      " |  ...     'This is the first document.',\n",
      " |  ...     'This document is the second document.',\n",
      " |  ...     'And this is the third one.',\n",
      " |  ...     'Is this the first document?',\n",
      " |  ... ]\n",
      " |  >>> vectorizer = TfidfVectorizer()\n",
      " |  >>> X = vectorizer.fit_transform(corpus)\n",
      " |  >>> print(vectorizer.get_feature_names())\n",
      " |  ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      " |  >>> print(X.shape)\n",
      " |  (4, 9)\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  CountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n",
      " |  \n",
      " |  TfidfTransformer : Performs the TF-IDF transformation from a provided\n",
      " |      matrix of counts.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The ``stop_words_`` attribute can get large and increase the model size\n",
      " |  when pickling. This attribute is provided only for introspection and can\n",
      " |  be safely removed using delattr or set to None before pickling.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TfidfVectorizer\n",
      " |      CountVectorizer\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      VectorizerMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer='word', stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.float64'>, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, raw_documents, y=None)\n",
      " |      Learn vocabulary and idf from training set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          an iterable which yields either str, unicode or file objects\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : TfidfVectorizer\n",
      " |  \n",
      " |  fit_transform(self, raw_documents, y=None)\n",
      " |      Learn vocabulary and idf, return term-document matrix.\n",
      " |      \n",
      " |      This is equivalent to fit followed by transform, but more efficiently\n",
      " |      implemented.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          an iterable which yields either str, unicode or file objects\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : sparse matrix, [n_samples, n_features]\n",
      " |          Tf-idf-weighted document-term matrix.\n",
      " |  \n",
      " |  transform(self, raw_documents, copy=True)\n",
      " |      Transform documents to document-term matrix.\n",
      " |      \n",
      " |      Uses the vocabulary and document frequencies (df) learned by fit (or\n",
      " |      fit_transform).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          an iterable which yields either str, unicode or file objects\n",
      " |      \n",
      " |      copy : boolean, default True\n",
      " |          Whether to copy X and operate on the copy or perform in-place\n",
      " |          operations.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : sparse matrix, [n_samples, n_features]\n",
      " |          Tf-idf-weighted document-term matrix.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  idf_\n",
      " |  \n",
      " |  norm\n",
      " |  \n",
      " |  smooth_idf\n",
      " |  \n",
      " |  sublinear_tf\n",
      " |  \n",
      " |  use_idf\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from CountVectorizer:\n",
      " |  \n",
      " |  get_feature_names(self)\n",
      " |      Array mapping from feature integer indices to feature name\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Return terms per document with nonzero entries in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array, sparse matrix}, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_inv : list of arrays, len = n_samples\n",
      " |          List of arrays of terms.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from VectorizerMixin:\n",
      " |  \n",
      " |  build_analyzer(self)\n",
      " |      Return a callable that handles preprocessing and tokenization\n",
      " |  \n",
      " |  build_preprocessor(self)\n",
      " |      Return a function to preprocess the text before tokenization\n",
      " |  \n",
      " |  build_tokenizer(self)\n",
      " |      Return a function that splits a string into a sequence of tokens\n",
      " |  \n",
      " |  decode(self, doc)\n",
      " |      Decode the input into a string of unicode symbols\n",
      " |      \n",
      " |      The decoding strategy depends on the vectorizer parameters.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      doc : string\n",
      " |          The string to decode\n",
      " |  \n",
      " |  get_stop_words(self)\n",
      " |      Build or fetch the effective stop words list\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_train = (train['Sub-issue'].isnull())\n",
    "condition_test= (test['Sub-issue'].isnull())\n",
    "cols = ['Sub-issue']\n",
    "train.loc[condition_train,cols] = ''\n",
    "test.loc[condition_test,cols] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date received                        0\n",
       "Product                              0\n",
       "Sub-product                          0\n",
       "Issue                                0\n",
       "Sub-issue                            0\n",
       "Consumer complaint narrative    403327\n",
       "Company public response         388029\n",
       "Company                              0\n",
       "State                             3839\n",
       "ZIP code                          3848\n",
       "Submitted via                        0\n",
       "Date sent to company                 0\n",
       "Company response to consumer         0\n",
       "Timely response?                     0\n",
       "Consumer disputed?                   0\n",
       "Complaint ID                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_sub_issue = tfidf.fit(train['Sub-issue'])\n",
    "train_tfidf = x_sub_issue.transform(train['Sub-issue'])\n",
    "test_tfidf = x_sub_issue.transform(test['Sub-issue'])\n",
    "\n",
    "train_features2 = pd.DataFrame(train_tfidf.toarray())\n",
    "test_features2 = pd.DataFrame(test_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(478421, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consumer complaint narrative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_train = (train['Consumer complaint narrative'].isnull())\n",
    "condition_test= (test['Consumer complaint narrative'].isnull())\n",
    "cols = ['Consumer complaint narrative']\n",
    "train.loc[condition_train,cols] = ''\n",
    "test.loc[condition_test,cols] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_complaint = tfidf.fit(train['Consumer complaint narrative'])\n",
    "train_tfidf = x_complaint.transform(train['Consumer complaint narrative'])\n",
    "test_tfidf = x_complaint.transform(test['Consumer complaint narrative'])\n",
    "\n",
    "train_features3 = pd.DataFrame(train_tfidf.toarray())\n",
    "test_features3 = pd.DataFrame(test_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(478421, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.reset_index()\n",
    "test=test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the new features to train and test tabels\n",
    "final_train = pd.concat([train,train_features1,train_features2,train_features3],axis=1)\n",
    "final_test = pd.concat([test,test_features1,test_features2,test_features3],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(478421, 32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping complaint id column from both test and train\n",
    "final_test.drop(['Complaint ID'],inplace=True,axis=1)\n",
    "final_train.drop(['Complaint ID'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping Company public response column from both test and train\n",
    "final_test.drop(['Company public response'],inplace=True,axis=1)\n",
    "final_train.drop(['Company public response'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Date received</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sub-product</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Sub-issue</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>Company</th>\n",
       "      <th>State</th>\n",
       "      <th>ZIP code</th>\n",
       "      <th>...</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2014-05-15</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>Billing statement</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Wells Fargo &amp; Company</td>\n",
       "      <td>MI</td>\n",
       "      <td>48342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2014-09-18</td>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>(CD) Certificate of deposit</td>\n",
       "      <td>Making/receiving payments, sending money</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Santander Bank US</td>\n",
       "      <td>PA</td>\n",
       "      <td>18042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2014-03-13</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Incorrect information on credit report</td>\n",
       "      <td>Account status</td>\n",
       "      <td></td>\n",
       "      <td>Equifax</td>\n",
       "      <td>CA</td>\n",
       "      <td>92427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.520699</td>\n",
       "      <td>0.520737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.511572</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2015-07-17</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>Billing statement</td>\n",
       "      <td></td>\n",
       "      <td>My credit card statement from US Bank, XXXX. X...</td>\n",
       "      <td>U.S. Bancorp</td>\n",
       "      <td>GA</td>\n",
       "      <td>305XX</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2014-11-20</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>Transaction issue</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Bank of America</td>\n",
       "      <td>MA</td>\n",
       "      <td>02127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index Date received                  Product                  Sub-product  \\\n",
       "0      0    2014-05-15              Credit card                  Credit card   \n",
       "1      1    2014-09-18  Bank account or service  (CD) Certificate of deposit   \n",
       "2      2    2014-03-13         Credit reporting             Credit reporting   \n",
       "3      3    2015-07-17              Credit card                  Credit card   \n",
       "4      4    2014-11-20              Credit card                  Credit card   \n",
       "\n",
       "                                      Issue       Sub-issue  \\\n",
       "0                         Billing statement                   \n",
       "1  Making/receiving payments, sending money                   \n",
       "2    Incorrect information on credit report  Account status   \n",
       "3                         Billing statement                   \n",
       "4                         Transaction issue                   \n",
       "\n",
       "                        Consumer complaint narrative                Company  \\\n",
       "0                                                     Wells Fargo & Company   \n",
       "1                                                         Santander Bank US   \n",
       "2                                                                   Equifax   \n",
       "3  My credit card statement from US Bank, XXXX. X...           U.S. Bancorp   \n",
       "4                                                           Bank of America   \n",
       "\n",
       "  State ZIP code  ...    5         6         7    8    9   10        11   12  \\\n",
       "0    MI    48342  ...  0.0  0.000000  0.000000  0.0  0.0  0.0  0.000000  0.0   \n",
       "1    PA    18042  ...  0.0  0.000000  0.000000  0.0  0.0  1.0  0.000000  0.0   \n",
       "2    CA    92427  ...  0.0  0.520699  0.520737  0.0  0.0  0.0  0.511572  0.0   \n",
       "3    GA    305XX  ...  0.0  0.000000  0.000000  0.0  0.0  0.0  0.000000  0.0   \n",
       "4    MA    02127  ...  0.0  0.000000  0.000000  0.0  0.0  0.0  0.000000  0.0   \n",
       "\n",
       "     0    0  \n",
       "0  0.0  0.0  \n",
       "1  0.0  0.0  \n",
       "2  0.0  0.0  \n",
       "3  0.0  1.0  \n",
       "4  0.0  0.0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                              0\n",
       "Date received                      0\n",
       "Product                            0\n",
       "Sub-product                        0\n",
       "Issue                              0\n",
       "Sub-issue                          0\n",
       "Consumer complaint narrative       0\n",
       "Company                            0\n",
       "State                           3839\n",
       "ZIP code                        3848\n",
       "Submitted via                      0\n",
       "Date sent to company               0\n",
       "Company response to consumer       0\n",
       "Timely response?                   0\n",
       "Consumer disputed?                 0\n",
       "0                                  0\n",
       "1                                  0\n",
       "2                                  0\n",
       "3                                  0\n",
       "4                                  0\n",
       "5                                  0\n",
       "6                                  0\n",
       "7                                  0\n",
       "8                                  0\n",
       "9                                  0\n",
       "10                                 0\n",
       "11                                 0\n",
       "12                                 0\n",
       "0                                  0\n",
       "0                                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping issue , Sub-issue and Consumer complaint narrative\n",
    "cols = ['Issue','Sub-issue','Consumer complaint narrative']\n",
    "final_train.drop(cols,inplace=True,axis=1)\n",
    "final_test.drop(cols,inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(478421, 27)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting date comlumns to datetime type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = ['Date sent to company','Date received']\n",
    "for i in date_cols:\n",
    "    final_train[i] = pd.to_datetime(final_train[i])\n",
    "    final_test[i] = pd.to_datetime(final_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train['days_diff'] = final_train['Date sent to company'] - final_train['Date received']\n",
    "\n",
    "final_train['days_diff'] = final_train['days_diff'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train['days_diff'] = final_train['days_diff'].str.replace(\"days\",'')\n",
    "\n",
    "final_train['days_diff'] = final_train['days_diff'].str.replace(\":00\",'')\n",
    "final_train['days_diff'] = final_train['days_diff'].str.replace(\"  \",'')\n",
    "\n",
    "final_train['days_diff'] = final_train['days_diff'].str.replace(\"00.000000000\",'')\n",
    "\n",
    "final_train['days_diff'] = final_train['days_diff'].str.replace(\"+\",'')\n",
    "condition_final_train = (final_train['days_diff']=='NaT')\n",
    "final_train.loc[condition_final_train,['days_diff']] = 0\n",
    "final_train['days_diff'] = pd.to_numeric(final_train['days_diff'])\n",
    "\n",
    "final_train = final_train[final_train['days_diff']>=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test['days_diff'] = final_test['Date sent to company'] - final_test['Date received']\n",
    "final_test['days_diff'] = final_test['days_diff'].astype(str)\n",
    "final_test['days_diff'] = final_test['days_diff'].str.replace(\"days\",'')\n",
    "\n",
    "final_test['days_diff'] = final_test['days_diff'].str.replace(\":00\",'')\n",
    "final_test['days_diff'] = final_test['days_diff'].str.replace(\"  \",'')\n",
    "\n",
    "final_test['days_diff'] = final_test['days_diff'].str.replace(\"00.000000000\",'')\n",
    "\n",
    "final_test['days_diff'] = final_test['days_diff'].str.replace(\"+\",'')\n",
    "\n",
    "condition_final_test = (final_test['days_diff']=='NaT')\n",
    "final_test.loc[condition_final_test,['days_diff']] = 0\n",
    "final_test['days_diff'] = pd.to_numeric(final_test['days_diff'])\n",
    "\n",
    "\n",
    "condition_final_test = final_test['days_diff']<=0\n",
    "final_test.loc[condition_final_test,['days_diff']]=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                              0\n",
       "Date received                      0\n",
       "Product                            0\n",
       "Sub-product                        0\n",
       "Company                            0\n",
       "State                           3828\n",
       "ZIP code                        3837\n",
       "Submitted via                      0\n",
       "Date sent to company               0\n",
       "Company response to consumer       0\n",
       "Timely response?                   0\n",
       "Consumer disputed?                 0\n",
       "0                                  0\n",
       "1                                  0\n",
       "2                                  0\n",
       "3                                  0\n",
       "4                                  0\n",
       "5                                  0\n",
       "6                                  0\n",
       "7                                  0\n",
       "8                                  0\n",
       "9                                  0\n",
       "10                                 0\n",
       "11                                 0\n",
       "12                                 0\n",
       "0                                  0\n",
       "0                                  0\n",
       "days_diff                          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                             0\n",
       "Date received                     0\n",
       "Product                           0\n",
       "Sub-product                       0\n",
       "Company                           0\n",
       "State                           925\n",
       "ZIP code                        926\n",
       "Submitted via                     1\n",
       "Date sent to company              0\n",
       "Company response to consumer      0\n",
       "Timely response?                  0\n",
       "0                                 0\n",
       "1                                 0\n",
       "2                                 0\n",
       "3                                 0\n",
       "4                                 0\n",
       "5                                 0\n",
       "6                                 0\n",
       "7                                 0\n",
       "8                                 0\n",
       "9                                 0\n",
       "10                                0\n",
       "11                                0\n",
       "12                                0\n",
       "0                                 0\n",
       "0                                 0\n",
       "days_diff                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droppig columns Company and ZIP code since there are lot of unique values\n",
    "for col in ['ZIP code','Company']:\n",
    "    final_train.drop([col],1,inplace=True)\n",
    "    final_test.drop([col],1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dropping the date cols and company public response\n",
    "final_train.drop(['Date received','Date sent to company'],inplace=True,axis=1)\n",
    "final_test.drop(['Date received','Date sent to company'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                             int64\n",
       "Product                          object\n",
       "Sub-product                      object\n",
       "State                            object\n",
       "Submitted via                    object\n",
       "Company response to consumer     object\n",
       "Timely response?                 object\n",
       "Consumer disputed?               object\n",
       "0                               float64\n",
       "1                               float64\n",
       "2                               float64\n",
       "3                               float64\n",
       "4                               float64\n",
       "5                               float64\n",
       "6                               float64\n",
       "7                               float64\n",
       "8                               float64\n",
       "9                               float64\n",
       "10                              float64\n",
       "11                              float64\n",
       "12                              float64\n",
       "0                               float64\n",
       "0                               float64\n",
       "days_diff                         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                             int64\n",
       "Product                          object\n",
       "Sub-product                      object\n",
       "State                            object\n",
       "Submitted via                    object\n",
       "Company response to consumer     object\n",
       "Timely response?                 object\n",
       "0                               float64\n",
       "1                               float64\n",
       "2                               float64\n",
       "3                               float64\n",
       "4                               float64\n",
       "5                               float64\n",
       "6                               float64\n",
       "7                               float64\n",
       "8                               float64\n",
       "9                               float64\n",
       "10                              float64\n",
       "11                              float64\n",
       "12                              float64\n",
       "0                               float64\n",
       "0                               float64\n",
       "days_diff                         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train['Consumer disputed?'] = (final_train['Consumer disputed?']=='Yes').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = final_train.select_dtypes(['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product\n",
      "-------------------\n",
      "Mortgage                   154979\n",
      "Debt collection             85157\n",
      "Credit reporting            79787\n",
      "Credit card                 56435\n",
      "Bank account or service     54091\n",
      "Consumer Loan               18487\n",
      "Student loan                14737\n",
      "Money transfers              3286\n",
      "Payday loan                  3202\n",
      "Prepaid card                 2226\n",
      "Other financial service       507\n",
      "Virtual currency                8\n",
      "Name: Product, dtype: int64\n",
      "=====================================================\n",
      "Sub-product\n",
      "-------------------\n",
      "Credit reporting                          79787\n",
      "Credit card                               74063\n",
      "Other mortgage                            61399\n",
      "Conventional fixed mortgage               47485\n",
      "Checking account                          38080\n",
      "Other (i.e. phone, health club, etc.)     25337\n",
      "I do not know                             17884\n",
      "Conventional adjustable mortgage (ARM)    17347\n",
      "FHA mortgage                              16094\n",
      "Non-federal student loan                  15069\n",
      "Medical                                   11364\n",
      "Vehicle loan                              10629\n",
      "Other bank product/service                 9847\n",
      "Payday loan                                7758\n",
      "Home equity loan or line of credit         7552\n",
      "Installment loan                           4780\n",
      "Savings account                            3398\n",
      "VA mortgage                                3255\n",
      "Mortgage                                   2972\n",
      "(CD) Certificate of deposit                2364\n",
      "Auto                                       2132\n",
      "International money transfer               1729\n",
      "Domestic (US) money transfer               1565\n",
      "Federal student loan                       1508\n",
      "Vehicle lease                              1472\n",
      "Federal student loan servicing             1444\n",
      "Reverse mortgage                           1333\n",
      "Personal line of credit                    1266\n",
      "General purpose card                       1044\n",
      "Second mortgage                             514\n",
      "Cashing a check without an account          402\n",
      "Title loan                                  292\n",
      "Payroll card                                268\n",
      "Mobile wallet                               234\n",
      "Gift or merchant card                       212\n",
      "Government benefit payment card             204\n",
      "Check cashing                               142\n",
      "ID prepaid card                             126\n",
      "Debt settlement                             123\n",
      "Other special purpose card                  110\n",
      "Money order                                  75\n",
      "Credit repair                                48\n",
      "Pawn loan                                    48\n",
      "Refund anticipation check                    44\n",
      "Travelers/Cashiers checks                  43\n",
      "Foreign currency exchange                    32\n",
      "Transit card                                 23\n",
      "Electronic Benefit Transfer / EBT card        5\n",
      "Name: Sub-product, dtype: int64\n",
      "=====================================================\n",
      "State\n",
      "-------------------\n",
      "CA    69050\n",
      "FL    45678\n",
      "TX    35458\n",
      "NY    32410\n",
      "GA    21048\n",
      "NJ    19161\n",
      "PA    16920\n",
      "IL    16753\n",
      "VA    15409\n",
      "MD    15126\n",
      "OH    14670\n",
      "NC    13218\n",
      "MI    12245\n",
      "AZ    10550\n",
      "WA     9866\n",
      "MA     9484\n",
      "CO     8124\n",
      "TN     7301\n",
      "MO     6136\n",
      "SC     5942\n",
      "NV     5760\n",
      "OR     5595\n",
      "CT     5531\n",
      "MN     5529\n",
      "IN     5223\n",
      "WI     5207\n",
      "AL     4892\n",
      "LA     4728\n",
      "KY     3372\n",
      "OK     3138\n",
      "      ...  \n",
      "DE     2485\n",
      "NH     2381\n",
      "NM     2355\n",
      "KS     2341\n",
      "MS     2170\n",
      "AR     2053\n",
      "IA     1914\n",
      "ID     1662\n",
      "ME     1651\n",
      "HI     1625\n",
      "RI     1615\n",
      "NE     1478\n",
      "WV     1216\n",
      "PR     1130\n",
      "VT      796\n",
      "MT      729\n",
      "SD      620\n",
      "AK      546\n",
      "WY      448\n",
      "ND      409\n",
      "AE      195\n",
      "AP      130\n",
      "VI      121\n",
      "GU       58\n",
      "FM       30\n",
      "MH       25\n",
      "MP       15\n",
      "AS       14\n",
      "PW        9\n",
      "AA        8\n",
      "Name: State, Length: 62, dtype: int64\n",
      "=====================================================\n",
      "Submitted via\n",
      "-------------------\n",
      "Web            308445\n",
      "Referral        91307\n",
      "Phone           34416\n",
      "Postal mail     31448\n",
      "Fax              7032\n",
      "Email             254\n",
      "Name: Submitted via, dtype: int64\n",
      "=====================================================\n",
      "Company response to consumer\n",
      "-------------------\n",
      "Closed with explanation            350442\n",
      "Closed with non-monetary relief     60411\n",
      "Closed with monetary relief         32483\n",
      "Closed without relief               14130\n",
      "Closed                              11257\n",
      "Closed with relief                   4178\n",
      "Untimely response                       1\n",
      "Name: Company response to consumer, dtype: int64\n",
      "=====================================================\n",
      "Timely response?\n",
      "-------------------\n",
      "Yes    464782\n",
      "No       8120\n",
      "Name: Timely response?, dtype: int64\n",
      "=====================================================\n"
     ]
    }
   ],
   "source": [
    "for i in cols:\n",
    "    print(i)\n",
    "    print(\"-------------------\")\n",
    "    print(final_train[i].value_counts())\n",
    "    print(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product\n",
      "-------------------\n",
      "Mortgage                   38904\n",
      "Debt collection            21503\n",
      "Credit reporting           20497\n",
      "Credit card                14255\n",
      "Bank account or service    13691\n",
      "Consumer Loan               4664\n",
      "Student loan                3717\n",
      "Payday loan                  829\n",
      "Money transfers              827\n",
      "Prepaid card                 587\n",
      "Other financial service      128\n",
      "Virtual currency               4\n",
      "Name: Product, dtype: int64\n",
      "=====================================================\n",
      "Sub-product\n",
      "-------------------\n",
      "Credit reporting                          20497\n",
      "Credit card                               18626\n",
      "Other mortgage                            15241\n",
      "Conventional fixed mortgage               12099\n",
      "Checking account                           9721\n",
      "Other (i.e. phone, health club, etc.)      6444\n",
      "I do not know                              4473\n",
      "Conventional adjustable mortgage (ARM)     4434\n",
      "FHA mortgage                               3992\n",
      "Non-federal student loan                   3834\n",
      "Medical                                    2909\n",
      "Vehicle loan                               2697\n",
      "Other bank product/service                 2452\n",
      "Payday loan                                1982\n",
      "Home equity loan or line of credit         1939\n",
      "Installment loan                           1213\n",
      "Savings account                             818\n",
      "Mortgage                                    759\n",
      "VA mortgage                                 743\n",
      "(CD) Certificate of deposit                 600\n",
      "Auto                                        521\n",
      "International money transfer                427\n",
      "Domestic (US) money transfer                404\n",
      "Federal student loan                        385\n",
      "Federal student loan servicing              371\n",
      "Vehicle lease                               350\n",
      "Reverse mortgage                            320\n",
      "Personal line of credit                     319\n",
      "General purpose card                        267\n",
      "Second mortgage                             136\n",
      "Cashing a check without an account          100\n",
      "Title loan                                   71\n",
      "Payroll card                                 68\n",
      "Mobile wallet                                66\n",
      "Gift or merchant card                        55\n",
      "Government benefit payment card              53\n",
      "ID prepaid card                              37\n",
      "Check cashing                                35\n",
      "Other special purpose card                   31\n",
      "Debt settlement                              30\n",
      "Money order                                  21\n",
      "Pawn loan                                    14\n",
      "Foreign currency exchange                    14\n",
      "Travelers/Cashiers checks                  13\n",
      "Credit repair                                10\n",
      "Transit card                                  9\n",
      "Refund anticipation check                     5\n",
      "Electronic Benefit Transfer / EBT card        1\n",
      "Name: Sub-product, dtype: int64\n",
      "=====================================================\n",
      "State\n",
      "-------------------\n",
      "CA    17498\n",
      "FL    11676\n",
      "TX     8958\n",
      "NY     8469\n",
      "GA     5410\n",
      "NJ     4861\n",
      "PA     4298\n",
      "IL     4281\n",
      "VA     4003\n",
      "OH     3800\n",
      "MD     3757\n",
      "NC     3261\n",
      "MI     3083\n",
      "AZ     2612\n",
      "WA     2451\n",
      "MA     2328\n",
      "CO     1998\n",
      "TN     1805\n",
      "SC     1547\n",
      "MO     1511\n",
      "NV     1507\n",
      "OR     1442\n",
      "MN     1389\n",
      "CT     1332\n",
      "IN     1294\n",
      "WI     1281\n",
      "AL     1172\n",
      "LA     1109\n",
      "KY      873\n",
      "OK      788\n",
      "      ...  \n",
      "UT      604\n",
      "DE      600\n",
      "NH      567\n",
      "IA      552\n",
      "MS      549\n",
      "KS      533\n",
      "AR      499\n",
      "HI      417\n",
      "ID      395\n",
      "ME      385\n",
      "RI      369\n",
      "NE      360\n",
      "WV      333\n",
      "PR      290\n",
      "MT      215\n",
      "VT      198\n",
      "SD      168\n",
      "AK      135\n",
      "WY      127\n",
      "ND       92\n",
      "AE       49\n",
      "AP       34\n",
      "VI       33\n",
      "GU       20\n",
      "FM        5\n",
      "MP        5\n",
      "MH        3\n",
      "AS        3\n",
      "AA        3\n",
      "PW        2\n",
      "Name: State, Length: 62, dtype: int64\n",
      "=====================================================\n",
      "Submitted via\n",
      "-------------------\n",
      "Web            78448\n",
      "Referral       22879\n",
      "Phone           8541\n",
      "Postal mail     7900\n",
      "Fax             1756\n",
      "Email             81\n",
      "Name: Submitted via, dtype: int64\n",
      "=====================================================\n",
      "Company response to consumer\n",
      "-------------------\n",
      "Closed with explanation            88446\n",
      "Closed with non-monetary relief    15370\n",
      "Closed with monetary relief         8263\n",
      "Closed without relief               3624\n",
      "Closed                              2862\n",
      "Closed with relief                  1040\n",
      "Untimely response                      1\n",
      "Name: Company response to consumer, dtype: int64\n",
      "=====================================================\n",
      "Timely response?\n",
      "-------------------\n",
      "Yes    117677\n",
      "No       1929\n",
      "Name: Timely response?, dtype: int64\n",
      "=====================================================\n"
     ]
    }
   ],
   "source": [
    "for i in cols:\n",
    "    print(i)\n",
    "    print(\"-------------------\")\n",
    "    print(final_test[i].value_counts())\n",
    "\n",
    "    print(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sub-product</th>\n",
       "      <th>State</th>\n",
       "      <th>Submitted via</th>\n",
       "      <th>Company response to consumer</th>\n",
       "      <th>Timely response?</th>\n",
       "      <th>Consumer disputed?</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>days_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>MI</td>\n",
       "      <td>Web</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>(CD) Certificate of deposit</td>\n",
       "      <td>PA</td>\n",
       "      <td>Referral</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>CA</td>\n",
       "      <td>Referral</td>\n",
       "      <td>Closed with non-monetary relief</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.442718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.520699</td>\n",
       "      <td>0.520737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.511572</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>GA</td>\n",
       "      <td>Web</td>\n",
       "      <td>Closed with monetary relief</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>MA</td>\n",
       "      <td>Web</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                  Product                  Sub-product State  \\\n",
       "0      0              Credit card                  Credit card    MI   \n",
       "1      1  Bank account or service  (CD) Certificate of deposit    PA   \n",
       "2      2         Credit reporting             Credit reporting    CA   \n",
       "3      3              Credit card                  Credit card    GA   \n",
       "4      4              Credit card                  Credit card    MA   \n",
       "\n",
       "  Submitted via     Company response to consumer Timely response?  \\\n",
       "0           Web          Closed with explanation              Yes   \n",
       "1      Referral                           Closed              Yes   \n",
       "2      Referral  Closed with non-monetary relief              Yes   \n",
       "3           Web      Closed with monetary relief              Yes   \n",
       "4           Web          Closed with explanation              Yes   \n",
       "\n",
       "   Consumer disputed?    0    1         2    3    4    5         6         7  \\\n",
       "0                   0  0.0  0.0  0.000000  0.0  0.0  0.0  0.000000  0.000000   \n",
       "1                   0  0.0  0.0  0.000000  0.0  0.0  0.0  0.000000  0.000000   \n",
       "2                   0  0.0  0.0  0.442718  0.0  0.0  0.0  0.520699  0.520737   \n",
       "3                   0  0.0  0.0  0.000000  0.0  0.0  0.0  0.000000  0.000000   \n",
       "4                   0  0.0  0.0  0.000000  0.0  0.0  0.0  0.000000  0.000000   \n",
       "\n",
       "     8    9   10        11   12    0    0  days_diff  \n",
       "0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0          1  \n",
       "1  0.0  0.0  1.0  0.000000  0.0  0.0  0.0          6  \n",
       "2  0.0  0.0  0.0  0.511572  0.0  0.0  0.0         21  \n",
       "3  0.0  0.0  0.0  0.000000  0.0  0.0  1.0          0  \n",
       "4  0.0  0.0  0.0  0.000000  0.0  0.0  0.0          8  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns',60)\n",
    "final_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train['Timely response?'] = (final_train['Timely response?']=='Yes').astype(int)\n",
    "final_test['Timely response?'] = (final_test['Timely response?']=='Yes').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=final_train['State'].value_counts()\n",
    "for val in k.axes[0][0:15]:\n",
    "    varname='State_'+val.replace(',','_').replace(' ','_')\n",
    "    final_train[varname]=np.where(final_train['State']==val,1,0)\n",
    "    final_test[varname]=np.where(final_test['State']==val,1,0)\n",
    "del final_train['State']\n",
    "del final_test['State']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['Sub-product']:\n",
    "    varname=col.replace('-','_').replace('?','').replace(\" \",'_')+'_isNan'\n",
    "    final_train[varname]=np.where(pd.isnull(final_train[col]),1,0)\n",
    "    final_train.drop([col],1,inplace=True)\n",
    "    final_test[varname]=np.where(pd.isnull(final_test[col]),1,0)\n",
    "    final_test.drop([col],1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['Product','Submitted via','Company response to consumer']:\n",
    "    \n",
    "    temp=pd.get_dummies(final_train[col],prefix=col,drop_first=True)\n",
    "    final_train=pd.concat([temp,final_train],1)\n",
    "    final_train.drop([col],1,inplace=True)\n",
    "    \n",
    "    temp=pd.get_dummies(final_test[col],prefix=col,drop_first=True)\n",
    "    final_test=pd.concat([temp,final_test],1)\n",
    "    final_test.drop([col],1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(472902, 57)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119606, 56)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    372676\n",
       "1    100226\n",
       "Name: Consumer disputed?, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train['Consumer disputed?'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting train into features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=final_train.drop(['Consumer disputed?'],axis=1)\n",
    "y=final_train['Consumer disputed?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=25,random_state=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "\n",
    "test = pca.transform(final_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Over sampling with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s, y_s = SMOTE(sampling_strategy=\"not majority\", random_state=2,\n",
    "                                k_neighbors=3).fit_resample(principalComponents, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x_s,y_s,test_size = 0.4,random_state = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### froming test and train from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_no_s,x_test_no_s,y_train_no_s,y_test_no_s = train_test_split(principalComponents,y,test_size = 0.3,random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression,LogisticRegressionCV\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LogisticRegression in module sklearn.linear_model.logistic object:\n",
      "\n",
      "class LogisticRegression(sklearn.base.BaseEstimator, sklearn.linear_model.base.LinearClassifierMixin, sklearn.linear_model.base.SparseCoefMixin)\n",
      " |  LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='warn', max_iter=100, multi_class='warn', verbose=0, warm_start=False, n_jobs=None)\n",
      " |  \n",
      " |  Logistic Regression (aka logit, MaxEnt) classifier.\n",
      " |  \n",
      " |  In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
      " |  scheme if the 'multi_class' option is set to 'ovr', and uses the cross-\n",
      " |  entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
      " |  (Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
      " |  'sag' and 'newton-cg' solvers.)\n",
      " |  \n",
      " |  This class implements regularized logistic regression using the\n",
      " |  'liblinear' library, 'newton-cg', 'sag' and 'lbfgs' solvers. It can handle\n",
      " |  both dense and sparse input. Use C-ordered arrays or CSR matrices\n",
      " |  containing 64-bit floats for optimal performance; any other input format\n",
      " |  will be converted (and copied).\n",
      " |  \n",
      " |  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
      " |  with primal formulation. The 'liblinear' solver supports both L1 and L2\n",
      " |  regularization, with a dual formulation only for the L2 penalty.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  penalty : str, 'l1' or 'l2', default: 'l2'\n",
      " |      Used to specify the norm used in the penalization. The 'newton-cg',\n",
      " |      'sag' and 'lbfgs' solvers support only l2 penalties.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |         l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
      " |  \n",
      " |  dual : bool, default: False\n",
      " |      Dual or primal formulation. Dual formulation is only implemented for\n",
      " |      l2 penalty with liblinear solver. Prefer dual=False when\n",
      " |      n_samples > n_features.\n",
      " |  \n",
      " |  tol : float, default: 1e-4\n",
      " |      Tolerance for stopping criteria.\n",
      " |  \n",
      " |  C : float, default: 1.0\n",
      " |      Inverse of regularization strength; must be a positive float.\n",
      " |      Like in support vector machines, smaller values specify stronger\n",
      " |      regularization.\n",
      " |  \n",
      " |  fit_intercept : bool, default: True\n",
      " |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      " |      added to the decision function.\n",
      " |  \n",
      " |  intercept_scaling : float, default 1.\n",
      " |      Useful only when the solver 'liblinear' is used\n",
      " |      and self.fit_intercept is set to True. In this case, x becomes\n",
      " |      [x, self.intercept_scaling],\n",
      " |      i.e. a \"synthetic\" feature with constant value equal to\n",
      " |      intercept_scaling is appended to the instance vector.\n",
      " |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      " |  \n",
      " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      " |      as all other features.\n",
      " |      To lessen the effect of regularization on synthetic feature weight\n",
      " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      " |  \n",
      " |  class_weight : dict or 'balanced', default: None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one.\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *class_weight='balanced'*\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional, default: None\n",
      " |      The seed of the pseudo random number generator to use when shuffling\n",
      " |      the data.  If int, random_state is the seed used by the random number\n",
      " |      generator; If RandomState instance, random_state is the random number\n",
      " |      generator; If None, the random number generator is the RandomState\n",
      " |      instance used by `np.random`. Used when ``solver`` == 'sag' or\n",
      " |      'liblinear'.\n",
      " |  \n",
      " |  solver : str, {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},              default: 'liblinear'.\n",
      " |  \n",
      " |      Algorithm to use in the optimization problem.\n",
      " |  \n",
      " |      - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
      " |        'saga' are faster for large ones.\n",
      " |      - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
      " |        handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
      " |        schemes.\n",
      " |      - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n",
      " |        'liblinear' and 'saga' handle L1 penalty.\n",
      " |  \n",
      " |      Note that 'sag' and 'saga' fast convergence is only guaranteed on\n",
      " |      features with approximately the same scale. You can\n",
      " |      preprocess the data with a scaler from sklearn.preprocessing.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         Stochastic Average Gradient descent solver.\n",
      " |      .. versionadded:: 0.19\n",
      " |         SAGA solver.\n",
      " |      .. versionchanged:: 0.20\n",
      " |          Default will change from 'liblinear' to 'lbfgs' in 0.22.\n",
      " |  \n",
      " |  max_iter : int, default: 100\n",
      " |      Useful only for the newton-cg, sag and lbfgs solvers.\n",
      " |      Maximum number of iterations taken for the solvers to converge.\n",
      " |  \n",
      " |  multi_class : str, {'ovr', 'multinomial', 'auto'}, default: 'ovr'\n",
      " |      If the option chosen is 'ovr', then a binary problem is fit for each\n",
      " |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      " |      across the entire probability distribution, *even when the data is\n",
      " |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      " |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      " |      and otherwise selects 'multinomial'.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      " |      .. versionchanged:: 0.20\n",
      " |          Default will change from 'ovr' to 'auto' in 0.22.\n",
      " |  \n",
      " |  verbose : int, default: 0\n",
      " |      For the liblinear and lbfgs solvers set verbose to any positive\n",
      " |      number for verbosity.\n",
      " |  \n",
      " |  warm_start : bool, default: False\n",
      " |      When set to True, reuse the solution of the previous call to fit as\n",
      " |      initialization, otherwise, just erase the previous solution.\n",
      " |      Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
      " |  \n",
      " |  n_jobs : int or None, optional (default=None)\n",
      " |      Number of CPU cores used when parallelizing over classes if\n",
      " |      multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
      " |      set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
      " |      not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors.\n",
      " |      See :term:`Glossary <n_jobs>` for more details.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  \n",
      " |  classes_ : array, shape (n_classes, )\n",
      " |      A list of class labels known to the classifier.\n",
      " |  \n",
      " |  coef_ : array, shape (1, n_features) or (n_classes, n_features)\n",
      " |      Coefficient of the features in the decision function.\n",
      " |  \n",
      " |      `coef_` is of shape (1, n_features) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
      " |      to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
      " |  \n",
      " |  intercept_ : array, shape (1,) or (n_classes,)\n",
      " |      Intercept (a.k.a. bias) added to the decision function.\n",
      " |  \n",
      " |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      " |      `intercept_` is of shape (1,) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `intercept_`\n",
      " |      corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
      " |      outcome 0 (False).\n",
      " |  \n",
      " |  n_iter_ : array, shape (n_classes,) or (1, )\n",
      " |      Actual number of iterations for all classes. If binary or multinomial,\n",
      " |      it returns only 1 element. For liblinear solver, only the maximum\n",
      " |      number of iteration across all classes is given.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |  \n",
      " |          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      " |          ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.linear_model import LogisticRegression\n",
      " |  >>> X, y = load_iris(return_X_y=True)\n",
      " |  >>> clf = LogisticRegression(random_state=0, solver='lbfgs',\n",
      " |  ...                          multi_class='multinomial').fit(X, y)\n",
      " |  >>> clf.predict(X[:2, :])\n",
      " |  array([0, 0])\n",
      " |  >>> clf.predict_proba(X[:2, :]) # doctest: +ELLIPSIS\n",
      " |  array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
      " |         [9.7...e-01, 2.8...e-02, ...e-08]])\n",
      " |  >>> clf.score(X, y)\n",
      " |  0.97...\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  SGDClassifier : incrementally trained logistic regression (when given\n",
      " |      the parameter ``loss=\"log\"``).\n",
      " |  LogisticRegressionCV : Logistic regression with built-in cross validation\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The underlying C implementation uses a random number generator to\n",
      " |  select features when fitting the model. It is thus not uncommon,\n",
      " |  to have slightly different results for the same input data. If\n",
      " |  that happens, try with a smaller tol parameter.\n",
      " |  \n",
      " |  Predict output may not match that of standalone liblinear in certain\n",
      " |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      " |  in the narrative documentation.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  LIBLINEAR -- A Library for Large Linear Classification\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
      " |  \n",
      " |  SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
      " |      Minimizing Finite Sums with the Stochastic Average Gradient\n",
      " |      https://hal.inria.fr/hal-00860051/document\n",
      " |  \n",
      " |  SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
      " |      SAGA: A Fast Incremental Gradient Method With Support\n",
      " |      for Non-Strongly Convex Composite Objectives\n",
      " |      https://arxiv.org/abs/1407.0202\n",
      " |  \n",
      " |  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
      " |      methods for logistic regression and maximum entropy models.\n",
      " |      Machine Learning 85(1-2):41-75.\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LogisticRegression\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.linear_model.base.LinearClassifierMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.linear_model.base.SparseCoefMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='warn', max_iter=100, multi_class='warn', verbose=0, warm_start=False, n_jobs=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          Training vector, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          Target vector relative to X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,) optional\n",
      " |          Array of weights that are assigned to individual samples.\n",
      " |          If not provided, then each sample is given unit weight.\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             *sample_weight* support to LogisticRegression.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Log of probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like, shape = [n_samples, n_classes]\n",
      " |          Returns the log-probability of the sample for each class in the\n",
      " |          model, where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      " |      the softmax function is used to find the predicted probability of\n",
      " |      each class.\n",
      " |      Else use a one-vs-rest approach, i.e calculate the probability\n",
      " |      of each class assuming it to be positive using the logistic function.\n",
      " |      and normalize these values across all the classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like, shape = [n_samples, n_classes]\n",
      " |          Returns the probability of the sample for each class in the model,\n",
      " |          where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model.base.LinearClassifierMixin:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Predict confidence scores for samples.\n",
      " |      \n",
      " |      The confidence score for a sample is the signed distance of that\n",
      " |      sample to the hyperplane.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      " |          Confidence scores per (sample, class) combination. In the binary\n",
      " |          case, confidence score for self.classes_[1] where >0 means this\n",
      " |          class would be predicted.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class labels for samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape [n_samples]\n",
      " |          Predicted class label per sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model.base.SparseCoefMixin:\n",
      " |  \n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator\n",
      " |  \n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |      \n",
      " |      The ``intercept_`` member is not converted.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |      \n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=LogisticRegression(fit_intercept=True,solver ='saga',n_jobs=-1,verbose=True,max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 793 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mangi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 13.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=2000, multi_class='warn',\n",
       "          n_jobs=-1, penalty='l2', random_state=None, solver='saga',\n",
       "          tol=0.0001, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method ClassifierMixin.score of LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=2000, multi_class='warn',\n",
       "          n_jobs=-1, penalty='l2', random_state=None, solver='saga',\n",
       "          tol=0.0001, verbose=True, warm_start=False)>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = l.predict(x_test_no_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5183119013811687"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(pred_test,y_test_no_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic using grid search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import Ridge,Lasso\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'penalty':['l1','l2'],\n",
    "        'C':np.linspace(0.1,0.3,10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LogisticRegression(fit_intercept=True)#,class_weight={0:0.2,1:0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search=GridSearchCV(model,param_grid=params,cv=5,scoring=\"roc_auc\",n_jobs=4,verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  1.0min\n",
      "C:\\Users\\mangi\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:  2.6min finished\n",
      "C:\\Users\\mangi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=4,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': array([0.1    , 0.12222, 0.14444, 0.16667, 0.18889, 0.21111, 0.23333,\n",
       "       0.25556, 0.27778, 0.3    ])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=True)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(x_train,y_train,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.18888888888888888, 'penalty': 'l1'}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search.predict(x_test_no_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5503519289250806"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_pred,y_test_no_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=np.where(grid_search.predict(test)==1,\"Yes\",\"No\")\n",
    "\n",
    "pd.DataFrame(prediction).head(4)\n",
    "\n",
    "pd.DataFrame(prediction).to_csv('sample_submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
